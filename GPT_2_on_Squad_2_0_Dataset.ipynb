{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TanzeelAbbas/DL_Files/blob/main/GPT_2_on_Squad_2_0_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_EY_oI1uv50"
      },
      "source": [
        "# **Load SQuAD dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B0Kz2fTxt_3u"
      },
      "outputs": [],
      "source": [
        "# ! pip install transformers datasets evaluate\n",
        "from datasets import load_dataset\n",
        "\n",
        "squad = load_dataset(\"squad\", split=\"train[:15000]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySI6mnijuA0L",
        "outputId": "173066c6-9878-44b9-e29d-9505c29bcd88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '56d5fbbd1c85041400946e95',\n",
              " 'title': 'Dog',\n",
              " 'context': 'Most breeds of dog are at most a few hundred years old, having been artificially selected for particular morphologies and behaviors by people for specific functional roles. Through this selective breeding, the dog has developed into hundreds of varied breeds, and shows more behavioral and morphological variation than any other land mammal. For example, height measured to the withers ranges from 15.2 centimetres (6.0 in) in the Chihuahua to about 76 cm (30 in) in the Irish Wolfhound; color varies from white through grays (usually called \"blue\") to black, and browns from light (tan) to dark (\"red\" or \"chocolate\") in a wide variation of patterns; coats can be short or long, coarse-haired to wool-like, straight, curly, or smooth. It is common for most breeds to shed this coat.',\n",
              " 'question': 'People selected dogs they wanted based on what two things?',\n",
              " 'answers': {'text': ['particular morphologies and behaviors'],\n",
              "  'answer_start': [94]}}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Split the dataset’s train split into a train and test set with the train_test_split method\n",
        "\n",
        "squad = squad.train_test_split(test_size=0.2, seed=2)\n",
        "\n",
        "squad[\"train\"][2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSee_FrRulDM"
      },
      "source": [
        "# **load a GPT-2 tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmVj2GfkxjPy",
        "outputId": "8fe4d49f-a062-455b-85bf-c3a6dbd900db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at gpt2 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Set the padding token to '[PAD]'\n",
        "tokenizer.pad_token = \"[PAD]\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnAxJQj1urp_"
      },
      "source": [
        "# **Few preprocessing steps to question answering**\n",
        "\n",
        "1. Some examples in a dataset may have a very long context that exceeds the maximum input length of the model. To deal with longer sequences, truncate only the context by setting truncation=\"only_second\".\n",
        "2. Next, map the start and end positions of the answer to the original context by setting return_offset_mapping=True.\n",
        "3. With the mapping in hand, now we can find the start and end tokens of the answer. Use the sequence_ids method to find which part of the offset corresponds to the question and which corresponds to the context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rEwCqFqezoGH"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=256,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    # Add a print statement here\n",
        "    # print(f\"Total examples: {len(offset_mapping)}\")\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        # Add another print statement here\n",
        "        # print(f\"Processing example {i + 1}/{len(offset_mapping)}\")\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        idx = 0\n",
        "        while idx < len(sequence_ids) and sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "\n",
        "        while idx < len(sequence_ids) and sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybQ6jwW1vaZ5"
      },
      "source": [
        "To apply the preprocessing function over the entire dataset, we use Datasets map function. We can speed up the map function by setting batched=True to process multiple elements of the dataset at once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "25l77Nuo0Vz7"
      },
      "outputs": [],
      "source": [
        "tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS5yDgFcvsnG"
      },
      "source": [
        "# **Setup HuggingFace token**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Nt-eRd5s6kM1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HF_HOME\"] = \"/root/.huggingface\"\n",
        "os.environ[\"HF_HOME\"] += \"/token\"\n",
        "os.environ[\"HF_HOME\"] = os.path.join(os.environ[\"HF_HOME\"], \"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\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc5FgPPHwVW-"
      },
      "source": [
        "### **Create a batch of examples using DefaultDataCollator.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qIU68_lqhJ4Y"
      },
      "outputs": [],
      "source": [
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UHtExj1seFa"
      },
      "source": [
        "# **Model Training**\n",
        "\n",
        "1. Define training hyperparameters in TrainingArguments. The only required parameter is output_dir which specifies where to save your model. We’ll push this model to the Hub by setting push_to_hub=True (we need to be signed in to Hugging Face to upload your model).\n",
        "2. Pass the training arguments to Trainer along with the model, dataset, tokenizer, and data collator.\n",
        "3. Call train() to finetune model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "R4tpIkvAg0ZS",
        "outputId": "52ef5083-6694-40aa-e316-a6b9630e54e1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2250/2250 33:55, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.205500</td>\n",
              "      <td>1.848124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.723500</td>\n",
              "      <td>1.668146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.580100</td>\n",
              "      <td>1.647874</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2250, training_loss=2.0649366853502062, metrics={'train_runtime': 2039.0639, 'train_samples_per_second': 17.655, 'train_steps_per_second': 1.103, 'total_flos': 4703341621248000.0, 'train_loss': 2.0649366853502062, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# !pip install accelerate -U\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=20,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_squad[\"train\"],\n",
        "    eval_dataset=tokenized_squad[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5671o3GDsXAS"
      },
      "source": [
        "# **Evaluate Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "auDGFOJfjlaw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "572ae996-13a6-4d6d-8d53-bffaaa50e14d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [188/188 00:49]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 1.647873878479004, 'eval_runtime': 49.6317, 'eval_samples_per_second': 60.445, 'eval_steps_per_second': 3.788}\n"
          ]
        }
      ],
      "source": [
        "# Define your data collator\n",
        "data_collator = DefaultDataCollator()\n",
        "\n",
        "# Define evaluation arguments\n",
        "evaluation_args = TrainingArguments(\n",
        "    per_device_eval_batch_size=16,  # Adjust batch size for evaluation if needed\n",
        "    output_dir=\"./evaluation_results\",  # Specify an output directory for evaluation results\n",
        ")\n",
        "\n",
        "# Create a Trainer for evaluation\n",
        "eval_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=evaluation_args,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "eval_results = eval_trainer.evaluate(tokenized_squad[\"test\"])\n",
        "\n",
        "# Print the evaluation results\n",
        "print(eval_results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "squad['test'][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nnRb213L1A4",
        "outputId": "9da27a01-ca7c-450b-b095-2a922920826b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '56cca27d6d243a140015f04c',\n",
              " 'title': 'IPod',\n",
              " 'context': \"With third parties like Namco, Square Enix, Electronic Arts, Sega, and Hudson Soft all making games for the iPod, Apple's MP3 player has taken steps towards entering the video game handheld console market. Even video game magazines like GamePro and EGM have reviewed and rated most of their games as of late.\",\n",
              " 'question': \"What are the names of companies producing video games for Apple's MP3 player?\",\n",
              " 'answers': {'text': ['Namco, Square Enix, Electronic Arts, Sega, and Hudson Soft'],\n",
              "  'answer_start': [24]}}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTg1DF0nsK1_"
      },
      "source": [
        "# **Predict Answer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DNQL8OFpVJm",
        "outputId": "2316ab2c-6240-45be-983d-2d1b5d42ed60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " third parties like Namco, Square Enix, Electronic Arts, Sega, and Hudson Soft\n"
          ]
        }
      ],
      "source": [
        "context = squad['test'][2]['context']\n",
        "question = squad['test'][2]['question']\n",
        "\n",
        "# Tokenize the question and context\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "\n",
        "# Move the inputs to the same device as the model\n",
        "inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
        "\n",
        "# Generate predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "\n",
        "# Find the answer span\n",
        "start_idx = torch.argmax(start_logits)\n",
        "end_idx = torch.argmax(end_logits)\n",
        "\n",
        "# Convert indices to Python integers\n",
        "start_idx = start_idx.item()\n",
        "end_idx = end_idx.item()\n",
        "\n",
        "# Tokenize the context and extract the answer span\n",
        "context_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "answer_tokens = context_tokens[start_idx:end_idx + 1]\n",
        "answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "\n",
        "print(answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffHEUzu7jeAe",
        "outputId": "8c4d55f7-78ec-4627-e510-b64664c50763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Exact Match (EM) Score: 0.42766666666666664\n",
            "Average F1 Score: 0.5768516913389369\n"
          ]
        }
      ],
      "source": [
        "# Load the evaluation dataset\n",
        "eval_dataset = squad[\"test\"]\n",
        "\n",
        "# Check if a GPU is available and use it if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Initialize variables to store scores\n",
        "total_em = 0\n",
        "total_f1 = 0\n",
        "total_examples = 0\n",
        "\n",
        "\n",
        "def calculate_em_f1(predicted, ground_truth):\n",
        "    # Calculate EM and F1 scores\n",
        "    predicted_tokens = set(predicted.lower().split())\n",
        "    ground_truth_tokens = set(ground_truth.lower().split())\n",
        "    common_tokens = predicted_tokens.intersection(ground_truth_tokens)\n",
        "\n",
        "    if len(predicted_tokens) == 0 or len(ground_truth_tokens) == 0:\n",
        "        f1_score = 0\n",
        "    else:\n",
        "        precision = len(common_tokens) / len(predicted_tokens)\n",
        "        recall = len(common_tokens) / len(ground_truth_tokens)\n",
        "        if precision + recall == 0:\n",
        "            f1_score = 0\n",
        "        else:\n",
        "            f1_score = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    exact_match = int((predicted.strip()).lower() == (ground_truth.strip()).lower())\n",
        "\n",
        "    return exact_match, f1_score\n",
        "\n",
        "# Loop through the evaluation dataset\n",
        "for example in eval_dataset:\n",
        "    # Get the context and question from the example\n",
        "    context = example[\"context\"]\n",
        "    question = example[\"question\"]\n",
        "\n",
        "    # Get the list of answers from the example\n",
        "    answers = example.get(\"answers\", [{\"text\": [\"\"]}])  # Default to an empty list\n",
        "\n",
        "    # Inside the loop, move inputs and outputs to the same device\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits = outputs.end_logits\n",
        "\n",
        "    # Find the answer span\n",
        "    start_idx = torch.argmax(start_logits)\n",
        "    end_idx = torch.argmax(end_logits)\n",
        "\n",
        "    # Convert indices to Python integers\n",
        "    start_idx = start_idx.item()\n",
        "    end_idx = end_idx.item()\n",
        "\n",
        "    # Tokenize the context and extract the predicted answer span\n",
        "    context_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "    predicted_answer_tokens = context_tokens[start_idx:end_idx + 1]\n",
        "    predicted_answer = tokenizer.convert_tokens_to_string(predicted_answer_tokens)\n",
        "\n",
        "    # Get the ground truth answer (it's a list, so we'll choose the first answer)\n",
        "    ground_truth_answer = answers[\"text\"][0]\n",
        "\n",
        "    em, f1 = calculate_em_f1(predicted_answer, ground_truth_answer)\n",
        "\n",
        "    total_em += em\n",
        "    total_f1 += f1\n",
        "    total_examples += 1\n",
        "\n",
        "# Calculate average EM and F1 scores\n",
        "average_em = total_em / total_examples if total_examples > 0 else 0\n",
        "average_f1 = total_f1 / total_examples if total_examples > 0 else 0\n",
        "\n",
        "print(\"Average Exact Match (EM) Score:\", average_em)\n",
        "print(\"Average F1 Score:\", average_f1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"QA_finetuned_model\")"
      ],
      "metadata": {
        "id": "sWUol_onMd9h"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vHIwNAZQMrlN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}